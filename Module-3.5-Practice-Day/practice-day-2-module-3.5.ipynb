{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59bf2716-3e41-4793-9d83-51b3dc612932",
   "metadata": {},
   "source": [
    "# Part A. Quick basics\n",
    "\n",
    "## A1. Spot the right scaler\n",
    "\n",
    "| Feature                         | Scaler         | Reason                                                                 |\n",
    "|--------------------------------|----------------|------------------------------------------------------------------------|\n",
    "| Apartment_price_BDT (luxury)    | Robust Scaler  | আউটলাইয়ার আছে, তাই Robust Scaler আউটলাইয়ার থেকে ভালো সুরক্ষা দেয়।       |\n",
    "| Skin_temperature_C (30 to 36)  | Min-Max Scaler | ছোট এবং নির্দিষ্ট রেঞ্জ, Min-Max ভালো কাজ করবে।                          |\n",
    "| Daily_app_opens (many zeros)   | Robust Scaler  | অনেক শূন্য এবং আউটলাইয়ার, Robust Scaler ভালো মানায়।                      |\n",
    "\n",
    "---\n",
    "\n",
    "## A2. Manual Min-Max on a tiny set\n",
    "\n",
    "Given scores = `[20, 25, 30, 50]`\n",
    "\n",
    "- Min = 20  \n",
    "- Max = 50\n",
    "\n",
    "Formula:  \n",
    "\\[\n",
    "x_{scaled} = \\frac{x - Min}{Max - Min}\n",
    "\\]\n",
    "\n",
    "Calculations:\n",
    "\n",
    "| Original | Calculation               | Scaled Value |\n",
    "|----------|---------------------------|--------------|\n",
    "| 20       | (20 - 20) / (50 - 20) = 0 | 0            |\n",
    "| 25       | (25 - 20) / 30 = 5/30      | 0.1667       |\n",
    "| 30       | (30 - 20) / 30 = 10/30     | 0.3333       |\n",
    "| 50       | (50 - 20) / 30 = 30/30     | 1            |\n",
    "\n",
    "---\n",
    "\n",
    "## A3. Z-scores on a subset\n",
    "\n",
    "Given \\(x = [8, 9, 11]\\)\n",
    "\n",
    "1. **Mean:**  \n",
    "\\[\n",
    "\\bar{x} = \\frac{8 + 9 + 11}{3} = 9.3333\n",
    "\\]\n",
    "\n",
    "2. **Population standard deviation:**  \n",
    "\\[\n",
    "\\sigma = \\sqrt{\\frac{(8-9.333)^2 + (9-9.333)^2 + (11-9.333)^2}{3}} = 1.2472\n",
    "\\]\n",
    "\n",
    "3. **Z-scores:**  \n",
    "\\[\n",
    "z = \\frac{x_i - \\bar{x}}{\\sigma}\n",
    "\\]\n",
    "\n",
    "| Value | Z-score Calculation                  | Z-score  |\n",
    "|-------|------------------------------------|----------|\n",
    "| 8     | (8 - 9.333) / 1.2472 = -1.069      | -1.069   |\n",
    "| 9     | (9 - 9.333) / 1.2472 = -0.267      | -0.267   |\n",
    "| 11    | (11 - 9.333) / 1.2472 = 1.337      | 1.337    |\n",
    "\n",
    "---\n",
    "\n",
    "## A4. Robust scaling ingredients\n",
    "\n",
    "Given \\(y = [5, 6, 6, 7, 50]\\)\n",
    "\n",
    "- **Median:** 6  \n",
    "- **Q1 (First Quartile):** median of [5, 6] = 5.5  \n",
    "- **Q3 (Third Quartile):** median of [7, 50] = 28.5  \n",
    "- **IQR (Interquartile Range):** \\(28.5 - 5.5 = 23\\)\n",
    "\n",
    "---\n",
    "\n",
    "## A5. Nominal or ordinal\n",
    "\n",
    "| Variable                 | Type    | Reason                                                   |\n",
    "|--------------------------|---------|----------------------------------------------------------|\n",
    "| T-shirt_size {S, M, L, XL} | Ordinal | স্পষ্ট ক্রম আছে (S < M < L < XL)                        |\n",
    "| City {Dhaka, Chattogram, Rajshahi} | Nominal | কোনো অর্ডার বা মান নেই                                  |\n",
    "| Satisfaction {Low, Medium, High} | Ordinal | স্পষ্ট ক্রম আছে (Low < Medium < High)                   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82a59f95-143f-4a76-b177-35b6403d16bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max scaled heights: [0.         0.33333333 0.66666667 0.83333333 1.        ]\n",
      "Min-Max scaled weights: [0.         0.03030303 0.0530303  0.06060606 1.        ]\n",
      "\n",
      "Standardized first 3 heights: [-1.22474487  0.          1.22474487]\n",
      "Standardized first 3 weights: [-1.27872403  0.11624764  1.16247639]\n",
      "\n",
      "Robust scaled weights: [-1.75 -0.75  0.    0.25 31.25]\n",
      "   City_Chattogram  City_Dhaka  City_Rajshahi\n",
      "0            False        True          False\n",
      "1             True       False          False\n",
      "2            False        True          False\n",
      "3            False       False           True\n",
      "4            False       False           True\n",
      "     Education  Education_mapped\n",
      "0  High School                 0\n",
      "1     Bachelor                 1\n",
      "2       Master                 2\n",
      "3     Bachelor                 1\n",
      "4       Master                 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# B1.a) Min-Max scale both to [0, 1] (হাত দিয়ে)\n",
    "def min_max_scale(arr):\n",
    "    min_val = np.min(arr)\n",
    "    max_val = np.max(arr)\n",
    "    return (arr - min_val) / (max_val - min_val)\n",
    "\n",
    "heights = np.array([150, 160, 170, 175, 180])\n",
    "weights = np.array([58, 62, 65, 66, 190])\n",
    "\n",
    "heights_minmax = min_max_scale(heights)\n",
    "weights_minmax = min_max_scale(weights)\n",
    "\n",
    "print(\"Min-Max scaled heights:\", heights_minmax)\n",
    "print(\"Min-Max scaled weights:\", weights_minmax)\n",
    "\n",
    "# B1.b) Standardize the first three values of each only (হাত দিয়ে)\n",
    "def standardize(arr):\n",
    "    mean = np.mean(arr)\n",
    "    std = np.std(arr)  # population std deviation (ddof=0)\n",
    "    return (arr - mean) / std\n",
    "\n",
    "heights_std = standardize(heights[:3])\n",
    "weights_std = standardize(weights[:3])\n",
    "\n",
    "print(\"\\nStandardized first 3 heights:\", heights_std)\n",
    "print(\"Standardized first 3 weights:\", weights_std)\n",
    "\n",
    "# B1.c) Robust scale Weights with median and IQR (হাত দিয়ে)\n",
    "def robust_scale(arr):\n",
    "    median = np.median(arr)\n",
    "    q1 = np.percentile(arr, 25)\n",
    "    q3 = np.percentile(arr, 75)\n",
    "    iqr = q3 - q1\n",
    "    return (arr - median) / iqr\n",
    "\n",
    "weights_robust = robust_scale(weights)\n",
    "print(\"\\nRobust scaled weights:\", weights_robust)\n",
    "\n",
    "# B1.d) Which scaler handles the outlier best?\n",
    "# # Robust scaler আউটলাইয়ারকে সবচেয়ে ভালো হ্যান্ডেল করে কারণ এটি median এবং IQR ব্যবহার করে।\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# B2. One-hot by hand\n",
    "import pandas as pd\n",
    "\n",
    "cities = ['Dhaka', 'Chattogram', 'Dhaka', 'Rajshahi', 'Rajshahi']\n",
    "df = pd.DataFrame({'City': cities})\n",
    "\n",
    "one_hot_df = pd.get_dummies(df['City'], prefix='City')\n",
    "print(one_hot_df)\n",
    "\n",
    "# B3. Ordinal mapping\n",
    "education = ['High School', 'Bachelor', 'Master', 'Bachelor', 'Master']\n",
    "df = pd.DataFrame({'Education': education})\n",
    "\n",
    "map1 = {'High School': 0, 'Bachelor': 1, 'Master': 2}\n",
    "df['Education_mapped'] = df['Education'].map(map1).astype(int)\n",
    "print(df)\n",
    "\n",
    "# Explanation:\n",
    "# Increasing all mapped values by 1 shifts the scale by a constant,\n",
    "# but relative distances between categories remain unchanged.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ad8a243-9857-4a8d-9b7d-08b9bdc68f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# B5.a) Dot products:\n",
      "a·b = 8\n",
      "a·c = -28\n",
      "\n",
      "# B5.c) L2 normalized vector a: [ 0.802 -0.267  0.535]\n",
      "\n",
      "# B6.a) Euclidean and Manhattan distances for all pairs:\n",
      "Pair 1: Euclidean = 5.000, Manhattan = 7\n",
      "Pair 2: Euclidean = 4.243, Manhattan = 6\n",
      "Pair 3: Euclidean = 7.000, Manhattan = 7\n",
      "\n",
      "# B6.c) After scaling y by 10:\n",
      "Euclidean distance between P1 and P2: 40.112\n",
      "Manhattan distance between P1 and P2: 43\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# B4. Encoding mixup [Optional]\n",
    "# You mistakenly apply ordinal encoding to City and one-hot to Education.\n",
    "# Write one sentence on the risk this creates in a linear model.\n",
    "\n",
    "# # কমেন্ট: \n",
    "# # Applying ordinal encoding to City falsely implies an order or hierarchy among cities,\n",
    "# # which can mislead the linear model into interpreting meaningless relationships.\n",
    "# # One-hot encoding Education removes the natural order between levels,\n",
    "# # potentially losing important ordinal information.\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "# B5. Vectors and alignment [Optional]\n",
    "a = np.array([3, -1, 2])\n",
    "b = np.array([4, 0, -2])\n",
    "c = np.array([-6, 2, -4])\n",
    "\n",
    "# a) Compute dot products\n",
    "dot_ab = np.dot(a, b)\n",
    "dot_ac = np.dot(a, c)\n",
    "\n",
    "print(\"# B5.a) Dot products:\")\n",
    "print(\"a·b =\", dot_ab)\n",
    "print(\"a·c =\", dot_ac)\n",
    "\n",
    "# b) Compare signs and magnitudes to comment on alignment\n",
    "# # dot_ab is positive (2), indicating a slight alignment between vectors a and b.\n",
    "# # dot_ac is negative (-28), indicating a strong opposite alignment between vectors a and c.\n",
    "\n",
    "# c) L2 normalize a and give the normalized vector to three decimals\n",
    "norm_a = np.linalg.norm(a)\n",
    "a_normalized = a / norm_a\n",
    "a_normalized_rounded = np.round(a_normalized, 3)\n",
    "\n",
    "print(\"\\n# B5.c) L2 normalized vector a:\", a_normalized_rounded)\n",
    "\n",
    "# ------------------------------------------------------------------------------------\n",
    "\n",
    "# B6. Two distances, different vibes\n",
    "P1 = np.array([2, 3])\n",
    "P2 = np.array([5, 7])\n",
    "P3 = np.array([2, 10])\n",
    "\n",
    "def euclidean_dist(p, q):\n",
    "    return np.linalg.norm(p - q)\n",
    "\n",
    "def manhattan_dist(p, q):\n",
    "    return np.sum(np.abs(p - q))\n",
    "\n",
    "pairs = [(P1, P2), (P2, P3), (P1, P3)]\n",
    "\n",
    "print(\"\\n# B6.a) Euclidean and Manhattan distances for all pairs:\")\n",
    "for i, (p, q) in enumerate(pairs, 1):\n",
    "    euc = euclidean_dist(p, q)\n",
    "    man = manhattan_dist(p, q)\n",
    "    print(f\"Pair {i}: Euclidean = {euc:.3f}, Manhattan = {man}\")\n",
    "\n",
    "# b) Which distance is more sensitive to a single large jump in one coordinate?\n",
    "# # Manhattan distance is more sensitive because it sums absolute coordinate differences,\n",
    "# # so a large change in one dimension directly increases the distance.\n",
    "\n",
    "# c) Scale y by 10 and recompute d(P1, P2)\n",
    "P1_scaled = np.array([2, 3*10])\n",
    "P2_scaled = np.array([5, 7*10])\n",
    "\n",
    "euc_scaled = euclidean_dist(P1_scaled, P2_scaled)\n",
    "man_scaled = manhattan_dist(P1_scaled, P2_scaled)\n",
    "\n",
    "print(f\"\\n# B6.c) After scaling y by 10:\")\n",
    "print(f\"Euclidean distance between P1 and P2: {euc_scaled:.3f}\")\n",
    "print(f\"Manhattan distance between P1 and P2: {man_scaled}\")\n",
    "\n",
    "# # Explanation:\n",
    "# # Scaling y by 10 significantly increases distances, especially Manhattan distance,\n",
    "# # because Manhattan distance adds coordinate differences directly,\n",
    "# # so large changes in one coordinate strongly affect the total distance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed169acf-eaaf-408f-9cf6-87f4b12bfd82",
   "metadata": {},
   "source": [
    "# Part C: Mini Datasets\n",
    "\n",
    "## C-Data-1\n",
    "\n",
    "| ID | Age | Hours_Study | GPA  | Internet | City       |\n",
    "|----|-----|-------------|------|----------|------------|\n",
    "| 1  | 20  | 1.0         | 3.10 | Yes      | Dhaka      |\n",
    "| 2  | 21  | 0.5         | 2.60 | No       | Chattogram |\n",
    "| 3  | 22  | 2.2         | 3.40 | Yes      | Rajshahi   |\n",
    "| 4  | 20  | 5.0         | 3.90 | Yes      | Dhaka      |\n",
    "| 5  | 23  | 0.2         | 2.30 | No       | Rajshahi   |\n",
    "\n",
    "---\n",
    "\n",
    "## C-Data-2\n",
    "\n",
    "| ID | Income_BDT | Transactions | Temp_C | Education    | Satisfaction |\n",
    "|----|------------|--------------|--------|--------------|--------------|\n",
    "| 1  | 30000      | 0            | 25.0   | High School  | Low          |\n",
    "| 2  | 45000      | 1            | 26.0   | Bachelor     | Medium       |\n",
    "| 3  | 52000      | 2            | 24.5   | Master       | High         |\n",
    "| 4  | 300000     | 12           | 28.0   | Bachelor     | Medium       |\n",
    "| 5  | 38000      | 0            | 25.5   | Master       | Medium       |\n",
    "\n",
    "---\n",
    "\n",
    "## C1. Scaler choices with evidence\n",
    "\n",
    "| Feature       | Chosen Scaler     | Justification                                    | Numeric Illustration                                                                                             |\n",
    "|---------------|-------------------|-------------------------------------------------|----------------------------------------------------------------------------------------------------------------|\n",
    "| **Income_BDT**    | Robust Scaler      | আউটলায়ার (300000) আছে, তাই Robust scaler ভালো কাজ করবে।       | Median ≈ 45000; IQR ≈ 47000 (38000 থেকে 85000); আউটলায়ার 300000 খুব বড়, Min-Max তে স্কেল নষ্ট হবে।                 |\n",
    "| **Transactions**  | Min-Max Scaler     | ছোট সংখ্যা, 0 থেকে 12 পর্যন্ত, স্প্রেড কম।                      | Min=0, Max=12, তাই Min-Max স্কেলিং সহজ ও কার্যকর। যেমন 0 → 0, 12 → 1।                                           |\n",
    "| **Temp_C**        | Standard Scaler    | পরিসর ছোট এবং প্রায় Gaussian distribution (প্রায় 24.5 থেকে 28) | Mean ≈ 25.8, Std ≈ 1.25; StandardScaler দিয়ে মানকে normalize করা যাবে।                                            |\n",
    "\n",
    "---\n",
    "\n",
    "## C2. Mixed preprocessing plan\n",
    "\n",
    "### a) Nominal ও Ordinal কলাম নির্ধারণ\n",
    "\n",
    "| Nominal Columns  | Ordinal Columns       |\n",
    "|------------------|-----------------------|\n",
    "| Internet (Yes/No) | Education (High School < Bachelor < Master) |\n",
    "| City             | Satisfaction (Low < Medium < High)            |\n",
    "\n",
    "### b) Encoding plan\n",
    "\n",
    "| Encoding type    | Columns                             |\n",
    "|------------------|-----------------------------------|\n",
    "| One-hot encoding | Internet, City                    |\n",
    "| Ordinal encoding | Education, Satisfaction           |\n",
    "\n",
    "### c) Scaling plan\n",
    "\n",
    "| Scaler           | Columns                           |\n",
    "|------------------|----------------------------------|\n",
    "| Robust Scaler    | Income_BDT                       |\n",
    "| Min-Max Scaler   | Transactions, Hours_Study, Age   |\n",
    "| Standard Scaler  | Temp_C, GPA                     |\n",
    "\n",
    "---\n",
    "\n",
    "**বিঃদ্রঃ**  \n",
    "- Age এবং Hours_Study ছোট পরিসর ও নরমাল ডিস্ট্রিবিউশনের জন্য StandardScaler বা Min-MaxScaler ব্যবহার করা যায়, এখানে Min-Max দিয়েছি।  \n",
    "- GPA প্রায় Gaussian তাই StandardScaler ভালো।  \n",
    "- Internet ও City nominal, তাই encoding হবে One-hot দিয়ে।  \n",
    "- Education এবং Satisfaction ordinal, তাই Ordinal encoding হবে।\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "432fcde8-ac1d-45ff-9ec6-e2b73920de13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max Scaled Income: [0.    0.056 0.081 1.    0.03 ]\n",
      "Robust Scaled Income: [-1.071  0.     0.5   18.214 -0.5  ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Income_BDT values\n",
    "income = np.array([30000, 45000, 52000, 300000, 38000])\n",
    "\n",
    "# Min-Max Scaling\n",
    "income_min = income.min()\n",
    "income_max = income.max()\n",
    "income_minmax = (income - income_min) / (income_max - income_min)\n",
    "\n",
    "# Robust Scaling (using Median and IQR)\n",
    "median = np.median(income)\n",
    "Q1 = np.percentile(income, 25)\n",
    "Q3 = np.percentile(income, 75)\n",
    "IQR = Q3 - Q1\n",
    "income_robust = (income - median) / IQR\n",
    "\n",
    "print(\"Min-Max Scaled Income:\", np.round(income_minmax, 3))\n",
    "print(\"Robust Scaled Income:\", np.round(income_robust, 3))\n",
    "\n",
    "# # Comparison comment:\n",
    "# Min-Max scaling compresses most values into a narrow range because the outlier 300000 sets a very large max, \n",
    "# whereas Robust scaling centers data around the median and reduces outlier influence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "960be852-f797-4a99-b947-41273e89a817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance: 4.079\n",
      "Manhattan distance: 4.800\n",
      "Normalized Euclidean distance: 1.414\n",
      "Normalized Manhattan distance: 2.000\n"
     ]
    }
   ],
   "source": [
    "# Data for IDs 1 and 4\n",
    "import numpy as np\n",
    "hs = np.array([1.0, 5.0])  # Hours_Study for ID 1 and 4\n",
    "gpa = np.array([3.10, 3.90])  # GPA for ID 1 and 4\n",
    "\n",
    "# Points as vectors (Hours_Study, GPA)\n",
    "p1 = np.array([1.0, 3.10])  # ID 1\n",
    "p4 = np.array([5.0, 3.90])  # ID 4\n",
    "# a) Euclidean distance\n",
    "euclidean = np.linalg.norm(p1 - p4)\n",
    "# Actually for 2D points Euclidean is:\n",
    "euclidean = np.sqrt((hs[1]-hs[0])**2 + (gpa[1]-gpa[0])**2)\n",
    "\n",
    "# b) Manhattan distance\n",
    "manhattan = np.abs(hs[1] - hs[0]) + np.abs(gpa[1] - gpa[0])\n",
    "\n",
    "print(f\"Euclidean distance: {euclidean:.3f}\")\n",
    "print(f\"Manhattan distance: {manhattan:.3f}\")\n",
    "\n",
    "# c) Min-Max normalize Hours_Study and GPA\n",
    "hs_min, hs_max = hs.min(), hs.max()\n",
    "gpa_min, gpa_max = gpa.min(), gpa.max()\n",
    "\n",
    "hs_norm = (hs - hs_min) / (hs_max - hs_min)\n",
    "gpa_norm = (gpa - gpa_min) / (gpa_max - gpa_min)\n",
    "\n",
    "euclidean_norm = np.sqrt((hs_norm[1] - hs_norm[0])**2 + (gpa_norm[1] - gpa_norm[0])**2)\n",
    "manhattan_norm = np.abs(hs_norm[1] - hs_norm[0]) + np.abs(gpa_norm[1] - gpa_norm[0])\n",
    "\n",
    "print(f\"Normalized Euclidean distance: {euclidean_norm:.3f}\")\n",
    "print(f\"Normalized Manhattan distance: {manhattan_norm:.3f}\")\n",
    "\n",
    "# # Comment:\n",
    "# # Normalization scales features to the same range, preventing one feature from dominating the distance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e47b3e6-1a76-478d-83e3-0af2e640b5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distances BEFORE scaling:\n",
      "Pair 1: Euclidean = 15000.00, Manhattan = 15003.00\n",
      "Pair 2: Euclidean = 10000.00, Manhattan = 10002.00\n",
      "Pair 3: Euclidean = 5000.00, Manhattan = 5001.00\n",
      "\n",
      "Distances AFTER scaling:\n",
      "Pair 1: Euclidean = 0.81, Manhattan = 1.05\n",
      "Pair 2: Euclidean = 0.54, Manhattan = 0.70\n",
      "Pair 3: Euclidean = 0.27, Manhattan = 0.35\n",
      "\n",
      "Reflection:\n",
      "- RobustScaler handled outliers better for Transactions_7d, reducing the influence of large values.\n",
      "- Scaling changes the magnitude of distances but usually preserves relative closeness.\n",
      "- Scaling is essential for distance-based algorithms to prevent dominance of features with larger scales or outliers.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "# Step 1: Create DataFrame\n",
    "data = {\n",
    "    \"Income\": [30000, 45000, 35000, 80000],\n",
    "    \"Hours_Study\": [1.0, 2.0, 1.5, 3.0],\n",
    "    \"GPA\": [3.1, 3.4, 3.0, 3.8],\n",
    "    \"Transactions_7d\": [0, 3, 1, 10],\n",
    "    \"City\": [\"Dhaka\", \"Chattogram\", \"Rajshahi\", \"Dhaka\"],\n",
    "    \"Internet\": [\"Yes\", \"No\", \"Yes\", \"No\"],\n",
    "    \"Education_Level\": [\"Bachelor\", \"Master\", \"High School\", \"Master\"],\n",
    "    \"Satisfaction\": [\"High\", \"Medium\", \"Low\", \"Medium\"]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 2: Encoding\n",
    "\n",
    "# One-hot encode City and Internet\n",
    "df_encoded = pd.get_dummies(df, columns=['City', 'Internet'])\n",
    "\n",
    "\n",
    "# Ordinal encode Education_Level and Satisfaction\n",
    "education_map = {\"High School\": 0, \"Bachelor\": 1, \"Master\": 2}\n",
    "satisfaction_map = {\"Low\": 0, \"Medium\": 1, \"High\": 2}\n",
    "\n",
    "df['Education_Level_encoded'] = df['Education_Level'].map(education_map)\n",
    "df['Satisfaction_encoded'] = df['Satisfaction'].map(satisfaction_map)\n",
    "\n",
    "# Step 3: Scaling\n",
    "\n",
    "# StandardScaler function\n",
    "def standard_scale(series):\n",
    "    mean = series.mean()\n",
    "    std = series.std(ddof=0)\n",
    "    return (series - mean) / std\n",
    "\n",
    "# MinMaxScaler function\n",
    "def minmax_scale(series):\n",
    "    min_val = series.min()\n",
    "    max_val = series.max()\n",
    "    return (series - min_val) / (max_val - min_val)\n",
    "\n",
    "# RobustScaler function\n",
    "def robust_scale(series):\n",
    "    median = series.median()\n",
    "    q1 = series.quantile(0.25)\n",
    "    q3 = series.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    return (series - median) / iqr\n",
    "\n",
    "df['Hours_Study_std'] = standard_scale(df['Hours_Study'])\n",
    "df['GPA_std'] = standard_scale(df['GPA'])\n",
    "df['Income_minmax'] = minmax_scale(df['Income'])\n",
    "df['Transactions_robust'] = robust_scale(df['Transactions_7d'])\n",
    "\n",
    "# Step 4: Distance calculations before and after scaling\n",
    "\n",
    "# Select first 3 rows\n",
    "P = df.iloc[:3]\n",
    "\n",
    "def euclidean_dist(row1, row2, cols):\n",
    "    return math.sqrt(sum((row1[c] - row2[c])**2 for c in cols))\n",
    "\n",
    "def manhattan_dist(row1, row2, cols):\n",
    "    return sum(abs(row1[c] - row2[c]) for c in cols)\n",
    "\n",
    "cols_before = ['Income', 'Transactions_7d']\n",
    "cols_after = ['Income_minmax', 'Transactions_robust']\n",
    "\n",
    "print(\"Distances BEFORE scaling:\")\n",
    "for i, (idx1, idx2) in enumerate([(0,1), (1,2), (0,2)], 1):\n",
    "    d_eu = euclidean_dist(P.loc[idx1], P.loc[idx2], cols_before)\n",
    "    d_ma = manhattan_dist(P.loc[idx1], P.loc[idx2], cols_before)\n",
    "    print(f\"Pair {i}: Euclidean = {d_eu:.2f}, Manhattan = {d_ma:.2f}\")\n",
    "\n",
    "print(\"\\nDistances AFTER scaling:\")\n",
    "for i, (idx1, idx2) in enumerate([(0,1), (1,2), (0,2)], 1):\n",
    "    d_eu = euclidean_dist(P.loc[idx1], P.loc[idx2], cols_after)\n",
    "    d_ma = manhattan_dist(P.loc[idx1], P.loc[idx2], cols_after)\n",
    "    print(f\"Pair {i}: Euclidean = {d_eu:.2f}, Manhattan = {d_ma:.2f}\")\n",
    "\n",
    "# Step 5: Reflection (print as comment)\n",
    "\n",
    "print(\"\"\"\n",
    "Reflection:\n",
    "- RobustScaler handled outliers better for Transactions_7d, reducing the influence of large values.\n",
    "- Scaling changes the magnitude of distances but usually preserves relative closeness.\n",
    "- Scaling is essential for distance-based algorithms to prevent dominance of features with larger scales or outliers.\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
